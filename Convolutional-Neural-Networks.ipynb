{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convolutional layers are comprised of filters and feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters\n",
    "\n",
    "* The filters are essentially the neurons of the layer. They have both weighted inputs and generate an output value like a neuron. The input size is a fixed square called a patch or a receptive field. If the convolutional layer is an input layer, then the input patch will be pixel values. If they deeper in the network architecture, then the convolutional layer will take input from a feature map from the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Maps\n",
    "\n",
    "* The feature map is the output of one filter applied to the previous layer. A given filter is drawn across the entire previous layer, moved one pixel at a time. Each position results in an activation of the neuron and the output is collected in the feature map.\n",
    "\n",
    "* The distance that filter is moved across the input from the previous layer each activation is referred to as the stride. If the size of the previous layer is not cleanly divisible by the size of the filters receptive field and the size of the stride then it is possible for the receptive field to attempt to read o↵ the edge of the input feature map. In this case, techniques like zero padding can be used to invent mock inputs with zero values for the receptive field to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "* The pooling layers down-sample the previous layers feature map. Pooling layers follow a sequence of one or more convolutional layers and are intended to consolidate the features learned and expressed in the previous layers feature map. As such, pooling may be consider a technique to compress or generalize feature representations and generally reduce the overfitting of the training data by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layers\n",
    "\n",
    "* Fully connected layers are the normal flat feedforward neural network layer. These layers may have a nonlinear activation function or a softmax activation in order to output probabilities of class predictions. Fully connected layers are used at the end of the network after feature extraction and consolidation has been performed by the convolutional and pooling layers. They are used to create final nonlinear combinations of features and for making predictions by the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Input Data\n",
    "\n",
    "* Let’s assume we have a dataset of gray scale images. Each image has the same size of 32 pixels wide and 32 pixels high, and pixel values are between 0 and 255, e.g. a matrix of 32 ⇥ 32 ⇥ 1 or 1,024 pixel values. Image input data is expressed as a 3-dimensional matrix of width ⇥ height ⇥ channels. If we were using color images in our example, we would have 3 channels for the red, green and blue pixel values, e.g. 32 ⇥ 32 ⇥ 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer\n",
    "\n",
    "* We define a convolutional layer with 10 filters and a receptive field 5 pixels wide and 5 pixels high and a stride length of 1. Because each filter can only get input from (i.e. see) 5 ⇥ 5 (25) pixels at a time, we can calculate that each will require 25 + 1 input weights (plus 1 for the bias input)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pool Layer\n",
    "* We define a pooling layer with a receptive field with a width of 2 inputs and a height of 2 inputs. We also use a stride of 2 to ensure that there is no overlap. This results in feature maps that are one half the size of the input feature maps. From 10 di↵erent 28 ⇥ 28 feature maps as input to 10 di↵erent 14 ⇥ 14 feature maps as output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer\n",
    "* Finally, we can flatten out the square feature maps into a traditional flat fully connected layer. We can define the fully connected layer with 200 hidden neurons, each with 10 ⇥ 14 ⇥ 14 input connections, or 1,960 + 1 weights per neuron. That is a total of 392,200 connections and weights to learn in this layer. We can use a sigmoid or softmax transfer function to output probabilities of class values directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Receptive Field Dimensions: \n",
    "* The default is 2D for images, but could be 1D such as for words in a sentence or 3D for video that adds a time dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receptive Field Size: \n",
    "* The patch should be as small as possible, but large enough to see features in the input data. It is common to use 3 ⇥ 3 on small images and 5 ⇥ 5 or 7 ⇥ 7 and more on larger image sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride Width:\n",
    "*  Use the default stride of 1. It is easy to understand and you don’t need padding to handle the receptive field falling o↵ the edge of your images. This could be increased to 2 or larger for larger images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Filters:\n",
    "* Filters are the feature detectors. Generally fewer filters are used at the input layer and increasingly more filters used at deeper layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding:\n",
    "* Set to zero and called zero padding when reading non-input data. This is useful when you cannot or do not want to standardize input image sizes or when you want to use receptive field and stride sizes that do not neatly divide up the input image size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling: \n",
    "* Pooling is a destructive or generalization process to reduce overfitting. Receptive field size is almost always set to 2 ⇥ 2 with a stride of 2 to discard 75% of the activations from the output of the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: \n",
    "* Consider standardizing input data, both the dimensions of the images and pixel values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Architecture: \n",
    "* It is common to pattern the layers in your network architecture. This might be one, two or some number of convolutional layers followed by a pooling layer. This structure can then be repeated one or more times. Finally, fully connected layers are often only used at the output end and may be stacked one, two or more deep.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout: \n",
    "* CNNs have a habit of overfitting, even with pooling layers. Dropout should be used such as between fully connected layers and perhaps after pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 18:09:17.386918: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 - 1s - loss: 0.2771 - accuracy: 0.9215 - val_loss: 0.1412 - val_accuracy: 0.9575 - 1s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "300/300 - 2s - loss: 0.1096 - accuracy: 0.9685 - val_loss: 0.0948 - val_accuracy: 0.9715 - 2s/epoch - 6ms/step\n",
      "Epoch 3/10\n",
      "300/300 - 2s - loss: 0.0715 - accuracy: 0.9785 - val_loss: 0.0774 - val_accuracy: 0.9759 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "300/300 - 1s - loss: 0.0502 - accuracy: 0.9851 - val_loss: 0.0719 - val_accuracy: 0.9766 - 1s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "300/300 - 1s - loss: 0.0365 - accuracy: 0.9895 - val_loss: 0.0650 - val_accuracy: 0.9797 - 1s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "300/300 - 1s - loss: 0.0265 - accuracy: 0.9931 - val_loss: 0.0673 - val_accuracy: 0.9795 - 1s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "300/300 - 1s - loss: 0.0206 - accuracy: 0.9946 - val_loss: 0.0651 - val_accuracy: 0.9801 - 1s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "300/300 - 1s - loss: 0.0167 - accuracy: 0.9956 - val_loss: 0.0588 - val_accuracy: 0.9807 - 1s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "300/300 - 1s - loss: 0.0105 - accuracy: 0.9980 - val_loss: 0.0584 - val_accuracy: 0.9835 - 1s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "300/300 - 1s - loss: 0.0087 - accuracy: 0.9983 - val_loss: 0.0623 - val_accuracy: 0.9815 - 1s/epoch - 5ms/step\n",
      "Baseline Error: 1.85%\n"
     ]
    }
   ],
   "source": [
    "# Baseline MLP for MNIST dataset\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(num_pixels, input_dim=num_pixels, activation= 'relu' ))\n",
    "  model.add(Dense(num_classes, activation= 'softmax' ))\n",
    "  # Compile model\n",
    "  model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=['accuracy'])\n",
    "  return model\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200,\n",
    "    verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Convolutional Neural Network for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 - 1s - loss: 1.1179 - accuracy: 0.6468 - val_loss: 0.5615 - val_accuracy: 0.8387 - 1s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "300/300 - 1s - loss: 0.6087 - accuracy: 0.8083 - val_loss: 0.4137 - val_accuracy: 0.8770 - 1s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "300/300 - 1s - loss: 0.5170 - accuracy: 0.8372 - val_loss: 0.3597 - val_accuracy: 0.8932 - 823ms/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "300/300 - 1s - loss: 0.4678 - accuracy: 0.8504 - val_loss: 0.3246 - val_accuracy: 0.9014 - 1s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "300/300 - 1s - loss: 0.4345 - accuracy: 0.8622 - val_loss: 0.3030 - val_accuracy: 0.9089 - 866ms/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "300/300 - 1s - loss: 0.4070 - accuracy: 0.8718 - val_loss: 0.2861 - val_accuracy: 0.9136 - 915ms/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "300/300 - 1s - loss: 0.3896 - accuracy: 0.8762 - val_loss: 0.2778 - val_accuracy: 0.9162 - 857ms/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "300/300 - 1s - loss: 0.3711 - accuracy: 0.8822 - val_loss: 0.2594 - val_accuracy: 0.9213 - 969ms/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "300/300 - 1s - loss: 0.3599 - accuracy: 0.8843 - val_loss: 0.2573 - val_accuracy: 0.9191 - 835ms/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "300/300 - 1s - loss: 0.3447 - accuracy: 0.8892 - val_loss: 0.2378 - val_accuracy: 0.9271 - 768ms/epoch - 3ms/step\n",
      "CNN Error: 7.29%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simple CNN for the MNIST Dataset\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][channels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28,28,1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28,28,1).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "# define a simple CNN model\n",
    "def baseline_model():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Convolution2D(32, 5, 5, input_shape=(28, 28, 1), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(num_classes, activation='softmax'))\n",
    "  # Compile model\n",
    "  model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "  return model\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200,\n",
    "    verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 - 8s - loss: 0.4186 - accuracy: 0.8647 - val_loss: 0.0841 - val_accuracy: 0.9752 - 8s/epoch - 28ms/step\n",
      "Epoch 2/10\n",
      "300/300 - 7s - loss: 0.1027 - accuracy: 0.9691 - val_loss: 0.0596 - val_accuracy: 0.9811 - 7s/epoch - 25ms/step\n",
      "Epoch 3/10\n",
      "300/300 - 7s - loss: 0.0731 - accuracy: 0.9773 - val_loss: 0.0537 - val_accuracy: 0.9830 - 7s/epoch - 24ms/step\n",
      "Epoch 4/10\n",
      "300/300 - 7s - loss: 0.0595 - accuracy: 0.9814 - val_loss: 0.0368 - val_accuracy: 0.9879 - 7s/epoch - 24ms/step\n",
      "Epoch 5/10\n",
      "300/300 - 8s - loss: 0.0510 - accuracy: 0.9842 - val_loss: 0.0355 - val_accuracy: 0.9885 - 8s/epoch - 26ms/step\n",
      "Epoch 6/10\n",
      "300/300 - 9s - loss: 0.0438 - accuracy: 0.9861 - val_loss: 0.0354 - val_accuracy: 0.9879 - 9s/epoch - 29ms/step\n",
      "Epoch 7/10\n",
      "300/300 - 9s - loss: 0.0392 - accuracy: 0.9879 - val_loss: 0.0306 - val_accuracy: 0.9899 - 9s/epoch - 31ms/step\n",
      "Epoch 8/10\n",
      "300/300 - 9s - loss: 0.0346 - accuracy: 0.9887 - val_loss: 0.0271 - val_accuracy: 0.9904 - 9s/epoch - 31ms/step\n",
      "Epoch 9/10\n",
      "300/300 - 9s - loss: 0.0329 - accuracy: 0.9890 - val_loss: 0.0313 - val_accuracy: 0.9891 - 9s/epoch - 29ms/step\n",
      "Epoch 10/10\n",
      "300/300 - 9s - loss: 0.0310 - accuracy: 0.9900 - val_loss: 0.0303 - val_accuracy: 0.9892 - 9s/epoch - 29ms/step\n",
      "CNN Error: 1.08%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simple CNN for the MNIST Dataset\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][channels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28,28,1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28,28,1).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "# define a simple CNN model\n",
    "def baseline_model():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Convolution2D(30,( 5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Convolution2D(15,(3,3), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(50, activation='relu'))\n",
    "  model.add(Dense(num_classes, activation='softmax'))\n",
    "  # Compile model\n",
    "  model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "  return model\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200,\n",
    "    verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
